{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sandal\", \"Dress\", \"Trouser\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAJiSURBVGje7djLi81hGMDxz4wz7gyTkXvEYpqNW4qUuxK5lPwBbgsWSlnaEQs2FvZ2LMhINqQpSi5hikRDFEoyyRjGZVwWzzuN5lCamFO/eb91mnOe+b3nOe/77Xmf9/cjk8lkMplMJpPJZDIZqvoyaBC+9YpNxFCMwJj0/6nowgoswnxU9/cMi5+w1JdBNcLRCYxFLT6nLxspHA/BW3wS/l5UaobFT9gnh5/S325HR/AGrcrrU4o/rNQMi5/wr/fSUvp1X7AMN/ERx7EWHWIvvYKLaBK1CXfTtYsrMcPiJ/wrh1X48cvnNtwR+2gTDqJe9MTtWIVOnMFhPEcL1ldihsVPWOawJPbD6vT6muKbcUv0vhuiFp+JfbL2N1+8AxtEX1wgPM6uxAyLn7DMYTW+94rtwQTMEN4O4R62YTfGY8kfxp/F8vR+XiVmWPyEZWea7vXfhdXCwTHsEz2tCu8xHc3YiPuiRhco979Q1PUo2eF/olTSc7/XJXpcM67hnbiPqMIFNOIJDqTBy/EI03BJ9Mxhes6tmzAar1N80oBY0v532CXcdTMcDXgp3HWm+Do8FXW1HudTfBPOiZrdkq6fK84wR/X4/C4cF39J+99ho3ByWzhqE3VTi9NYg5Pp4pO4jr3C4QjRH2twCldT/DLqhLcvep7btAyIJe1/h3XiGdgGjEvBegwWvW6p8DAZK4WHKXgl7v+qxZlzoqjDU9gp9tcP4lxbj8fCcfGXtPLn0pnCZQNmpdc48UymFe2iXltFb2zvNX4OtuKBqNMO4XN/pWZY/ISZTCaTyWQymUwm8y/4CTVBfjLjgmRXAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAPtSURBVGje7drLax1lGMfxT9K0aUxML0mbaoy2Falo0XqpN3RRBHGhWBDcuHChgkv/AHEluPKyci3uXIg3FKGIiFWstwZvVbGJbbX3JDatmtOkiYvfHOLiTOJC4jDkheGcZN4zzzzz5Xne3/M+02aR0YZH8AEOlczZUZybWOxiaP8Xc/7TUX+DbWX/nMNGvCJ87sD7+ExYbcQ5rMfN6MebxfxKeVh/g20LnXwBK3AUPZjBoLC7WFiO467i/F68hU8LT2ar4GH9DXYsdLIfZ7G/+L4CIziPdcUxhu+L+RfwsDCcLblm/R9pdRiuxu8Se9swKnnzT+FzOY6hD5uxT3Lq+qp5WH+DpQxvEk57sBsv44zE5fW4UeKvgRM4jS5hvlXitRIe1t9gKcMB0TU7Jb5el/XvUqzCATxYzH0DN2CTcL4XL1XFw/obLGXYg5XC5RvsEn16GL9gCi8Kx+3YIFrnIIaq5GH9DZYyHJS1bwDv4X6JyWexVtbBCXwhfC+RuuM3HKmSh/U3WMqwt7ibH3CV1BIzeAqv4VvRpWtEh36Fx9CJy6rkYf0NljI8jmtxD54WVjvxncTbuOjR2/E23sGTwntdceGZKnhYf4MtGfbKmvZXcUef4HEMY1I061FM40vhdkT0zjC6JdeeqIKH9TfYkuHdssY1RM90iUb9WOr8duF1heic+4rvP0ns/Ygtlhku0WjJcDVelTz5uTD9SDj24aTo1dMSf3twG97Fo5KHy5J0/R9pNRjukjy6W1heJPV+u8ThLcJyWHTorDA9JPHZkBpxbxU8rL/BlnveV+JOqSuewzNSZ3RJ3Tgm9cUpYdqHX6Wuf0JqyAOif/53D+tvsGUcHiyO5oS1sq/WJ3q0W2r8bRKHp2RPfBrPV83D+htsybCZM6elfjhefN4qcTgqvYyHpKc4Kvl2SPZJV0kP40IVPKy/wZYM/9k3akjCHTDfGx6RemK/9C5OSr9iZfGb6WJeJTysv8GOxSZM4zrRqn8Ux9X4WmrDa0S/9kgsMs+6Eh7W32Apw6bYGZT97E7Jlc26fqvk159lr3ROcmjlPKy/wUUZrimOc7JHc7b4e5Xoz0nz72l0F7+ZUx6L9X+k1WHYHFNSN4wUd9cjteN4cTSE34T5OJxb4Hr1f6TVYdjkcEb4NCRnfijaZUrWxg3CsCG9jcp5WH+DizJsyHsX+0R/bpbeUqf0myal/t8h8cqyplni0ZJhm/l3CztkT3RUasZJ6Vmcl9p/Cx6QvfFm73c5ly7pWDQOJ83XiCSHbio+x6VfcUz4DUm8sqxplnCUMmzW+WMSg/3Cs1f053bROQ3pOfXJvlvlPKy/wb8BktTloofuyDQAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAKHSURBVGje7dm7bhQxFMbx397CJkRBAUJBBWlSIQQFBfBCvATPwiNQUFIiAhWRAAESoIhrAuESks1tk12K42giKhdIsayMNJqds15//vYv28d2S+Z1B0McYBen0cMEzuJuZj3tXMH/ddUv2MotdICPOJNaOYFNbOASrmGpRIf1C3ZzCi1gBcvpfR/TOCX65vlUZqlEh/ULZjG8igv4jA4G2BHj6TjdC6U6rF8wi+FcenbEGNoTc2JX8NvH9VId1i+YxbCFvfScwLbg2cMIq5gt1WH9glkMdwTDXUyJPLSP9fT9gczk6Dgc1i+YxXCQCo5EDrOV7j7+YFJwLtJh/YJZDLc1DGfwFPdxT+Q5fSf98BgFsxgui/mvL+bDw3G0kyqYxo9SHdYvmMXwhcg/2+kHAzw40uJJvC/VYf2CWQyHIqfpij74K8VbYp0/ga+lOqxfMIshkbt0sIbfKTZKz56Ga3EO6xfMZrgmWI017IaiD37RcC3OYf2C2QxXcFmMn2dS7GdqcQvnSnVYv2A2w1XMC16z/1SwicelOqxfMJvhc9wSc+Jmim2InLSLd6U6rF8wmyGRg47xKr2/xE2xR3Oyxj82wWyGM6l1+3iUYg9xW+Q1N7BYosP6BbMZXhF5aFczH84dafG8E4bHJJjN8JsYR1viPJg4w9hLn7+X6rB+wWyGI81YupFiH1IFw5Id1i+YzfCTmPe2NOeGG6JfjjVci3NYv2A2w9eiv/U1a/x2ujuavZviHNYvmM1wcKR1h2NnS7Abi7mxSIf1C2YzHKfntuacaVOzf7qeWU/9f2m5DA/PnqZwEW/EPncrxSdLdVi/YDbDJ2J90cbbFFsUZxYjPCvVYf2CfwHrBHotWztMZwAAAABJRU5ErkJg\"></td></tr></tbody></table><div><small>(a vector displayed as a row to save space)</small></div>"
      ],
      "text/plain": [
       "3-element Array{Base.ReinterpretArray{Gray{Normed{UInt8,8}},2,Normed{UInt8,8},Array{Normed{UInt8,8},2}},1}:\n",
       " [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); … ; Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0)]\n",
       " [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); … ; Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0)]\n",
       " [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); … ; Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at some random images and labels in data\n",
    "using Knet,Images,Random,MLDatasets\n",
    "xtrn,ytrn = FashionMNIST.traindata(); ytrn .+= 1\n",
    "xtst,ytst = FashionMNIST.testdata();  ytst .+= 1\n",
    "lbls = FashionMNIST.classnames()\n",
    "rp = randperm(10000)\n",
    "println(lbls[ytst[rp[1:3]]]); flush(stdout)\n",
    "[ FashionMNIST.convert2image(xtst[:,:,rp[i]]) for i in 1:3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.FashionMLP"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"fashion-mnist.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "This example learns to classify images of fashion products(trousers, shirts, bags...)  from the \\href{https://github.com/zalandoresearch/fashion-mnist}{Fashion-MNIST} dataset.   There are 60000 training and 10000 test examples. Each input x  consists of 784 pixels representing a 28x28 image. The pixel values are  normalized to [0,1]. Each output y is converted to a ten-dimensional  one-hot vector (a vector that has a single non-zero component) indicating  the correct class (0-9) for a given image. 10 is used instead of 0. Labels and descriptions are shown below.\n",
       "\n",
       "\\begin{verbatim}\n",
       "Label   Description\n",
       "1       T-shirt/top\n",
       "2       Trouser\n",
       "3       Pullover\n",
       "4       Dress\n",
       "5       Coat\n",
       "6       Sandal\n",
       "7       Shirt\n",
       "8       Sneaker\n",
       "9       Bag\n",
       "10      Ankle boot\n",
       "\\end{verbatim}\n",
       "You can run the demo using \\texttt{julia fashion-mnist.jl} on the command line or by first including \\texttt{julia> include(\"fashion-mnist.jl\")} and typing \\texttt{julia> FashionMLP.main()}  at the Julia prompt.  Options can be used like \\texttt{julia fashion-mnist.jl --epochs 3}  or \\texttt{julia> FashionMLP.main(\"--epochs 3\")}. Use \\texttt{julia fashion-mnist.jl --help}  for a list of options.  The dataset will be automatically downloaded.   By default a softmax model will be trained for 10 epochs. You can also  train a multi-layer perceptron by specifying one or more –hidden sizes.  The accuracy for the training and test sets will be printed at every epoch  and optimized parameters will be returned.\n",
       "\n"
      ],
      "text/markdown": [
       "This example learns to classify images of fashion products(trousers, shirts, bags...)  from the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.   There are 60000 training and 10000 test examples. Each input x  consists of 784 pixels representing a 28x28 image. The pixel values are  normalized to [0,1]. Each output y is converted to a ten-dimensional  one-hot vector (a vector that has a single non-zero component) indicating  the correct class (0-9) for a given image. 10 is used instead of 0. Labels and descriptions are shown below.\n",
       "\n",
       "```\n",
       "Label   Description\n",
       "1       T-shirt/top\n",
       "2       Trouser\n",
       "3       Pullover\n",
       "4       Dress\n",
       "5       Coat\n",
       "6       Sandal\n",
       "7       Shirt\n",
       "8       Sneaker\n",
       "9       Bag\n",
       "10      Ankle boot\n",
       "```\n",
       "\n",
       "You can run the demo using `julia fashion-mnist.jl` on the command line or by first including `julia> include(\"fashion-mnist.jl\")` and typing `julia> FashionMLP.main()`  at the Julia prompt.  Options can be used like `julia fashion-mnist.jl --epochs 3`  or `julia> FashionMLP.main(\"--epochs 3\")`. Use `julia fashion-mnist.jl --help`  for a list of options.  The dataset will be automatically downloaded.   By default a softmax model will be trained for 10 epochs. You can also  train a multi-layer perceptron by specifying one or more –hidden sizes.  The accuracy for the training and test sets will be printed at every epoch  and optimized parameters will be returned.\n"
      ],
      "text/plain": [
       "  This example learns to classify images of fashion products(trousers, shirts, bags...) from the Fashion-MNIST\n",
       "  (https://github.com/zalandoresearch/fashion-mnist) dataset. There are 60000 training and 10000 test examples. Each input x consists of\n",
       "  784 pixels representing a 28x28 image. The pixel values are normalized to [0,1]. Each output y is converted to a ten-dimensional\n",
       "  one-hot vector (a vector that has a single non-zero component) indicating the correct class (0-9) for a given image. 10 is used instead\n",
       "  of 0. Labels and descriptions are shown below.\n",
       "\n",
       "\u001b[36m  Label   Description\u001b[39m\n",
       "\u001b[36m  1       T-shirt/top\u001b[39m\n",
       "\u001b[36m  2       Trouser\u001b[39m\n",
       "\u001b[36m  3       Pullover\u001b[39m\n",
       "\u001b[36m  4       Dress\u001b[39m\n",
       "\u001b[36m  5       Coat\u001b[39m\n",
       "\u001b[36m  6       Sandal\u001b[39m\n",
       "\u001b[36m  7       Shirt\u001b[39m\n",
       "\u001b[36m  8       Sneaker\u001b[39m\n",
       "\u001b[36m  9       Bag\u001b[39m\n",
       "\u001b[36m  10      Ankle boot\u001b[39m\n",
       "\n",
       "  You can run the demo using \u001b[36mjulia fashion-mnist.jl\u001b[39m on the command line or by first including \u001b[36mjulia> include(\"fashion-mnist.jl\")\u001b[39m and\n",
       "  typing \u001b[36mjulia> FashionMLP.main()\u001b[39m at the Julia prompt. Options can be used like \u001b[36mjulia fashion-mnist.jl --epochs 3\u001b[39m or \u001b[36mjulia>\n",
       "  FashionMLP.main(\"--epochs 3\")\u001b[39m. Use \u001b[36mjulia fashion-mnist.jl --help\u001b[39m for a list of options. The dataset will be automatically downloaded.\n",
       "  By default a softmax model will be trained for 10 epochs. You can also train a multi-layer perceptron by specifying one or more –hidden\n",
       "  sizes. The accuracy for the training and test sets will be printed at every epoch and optimized parameters will be returned."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc FashionMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: <PROGRAM> [--seed SEED] [--batchsize BATCHSIZE]\n",
      "                 [--epochs EPOCHS] [--hidden [HIDDEN...]] [--lr LR]\n",
      "                 [--winit WINIT] [--fast] [--atype ATYPE]\n",
      "                 [--gcheck GCHECK] [--dropout DROPOUT]\n",
      "\n",
      "fashion-mnist.jl (c) 2017 Adapted by Emre Unal based on Deniz Yuret’s\n",
      "MNIST example\n",
      "https://github.com/denizyuret/Knet.jl/tree/master/examples/mnist-mlp/mlp.jl.\n",
      "Multi-layer perceptron model on the Fashion-MNIST dataset from\n",
      "https://github.com/zalandoresearch/fashion-mnist.\n",
      "\n",
      "optional arguments:\n",
      "  --seed SEED           random number seed: use a nonnegative int for\n",
      "                        repeatable results (type: Int64, default: -1)\n",
      "  --batchsize BATCHSIZE\n",
      "                        minibatch size (type: Int64, default: 100)\n",
      "  --epochs EPOCHS       number of epochs for training (type: Int64,\n",
      "                        default: 10)\n",
      "  --hidden [HIDDEN...]  sizes of hidden layers, e.g. --hidden 128 64\n",
      "                        for a net with two hidden layers (type: Int64)\n",
      "  --lr LR               learning rate (type: Float64, default: 0.15)\n",
      "  --winit WINIT         w initialized with winit*randn() (type:\n",
      "                        Float64, default: 0.1)\n",
      "  --fast                skip loss printing for faster run\n",
      "  --atype ATYPE         array type: Array for cpu, KnetArray for gpu\n",
      "                        (default: \"KnetArray{Float32,N} where N\")\n",
      "  --gcheck GCHECK       check N random gradients per parameter (type:\n",
      "                        Int64, default: 0)\n",
      "  --dropout DROPOUT     Dropout probability. (type: Float64, default:\n",
      "                        0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FashionMLP.main(\"--help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-mnist.jl (c) 2017 Adapted by Emre Unal based on Deniz Yuret’s MNIST example https://github.com/denizyuret/Knet.jl/tree/master/examples/mnist-mlp/mlp.jl.\n",
      "Multi-layer perceptron model on the Fashion-MNIST dataset from https://github.com/zalandoresearch/fashion-mnist.\n",
      "\n",
      "opts=(:batchsize, 100)(:fast, false)(:atype, \"KnetArray{Float32,N} where N\")(:epochs, 10)(:gcheck, 0)(:winit, 0.1)(:dropout, 0.5)(:lr, 0.15)(:hidden, Int64[])(:seed, -1)\n",
      "(:epoch, 0, :trn, 0.08115, :tst, 0.0771)\n",
      "(:epoch, 1, :trn, 0.8052, :tst, 0.7966)\n",
      "(:epoch, 2, :trn, 0.8304666666666667, :tst, 0.8182)\n",
      "(:epoch, 3, :trn, 0.83985, :tst, 0.8274)\n",
      "(:epoch, 4, :trn, 0.8453833333333334, :tst, 0.8314)\n",
      "(:epoch, 5, :trn, 0.8497333333333333, :tst, 0.8341)\n",
      "(:epoch, 6, :trn, 0.8526166666666667, :tst, 0.8373)\n",
      "(:epoch, 7, :trn, 0.8551, :tst, 0.837)\n",
      "(:epoch, 8, :trn, 0.8566666666666667, :tst, 0.8374)\n",
      "(:epoch, 9, :trn, 0.8580166666666666, :tst, 0.8388)\n",
      "(:epoch, 10, :trn, 0.8589333333333333, :tst, 0.8396)\n",
      " 16.151464 seconds (24.18 M allocations: 6.064 GiB, 2.88% gc time)\n"
     ]
    }
   ],
   "source": [
    "model = FashionMLP.main(\"\");"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
